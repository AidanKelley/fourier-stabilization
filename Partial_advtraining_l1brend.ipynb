{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import torch \n",
    "import foolbox\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" )\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_float(x):\n",
    "    return x.astype(np.float32)\n",
    "\n",
    "def cast_int(x):\n",
    "    return x.astype(np.int32)\n",
    "\n",
    "def create_partition(x_orig_train, y_orig_train, p_train=0.2):\n",
    "  # inspired by pberkes answer: https://stackoverflow.com/questions/3674409/how-to-split-partition-a-dataset-into-training-and-test-datasets-for-e-g-cros, \n",
    "  # seed so we always get the same partition (can be changed later)\n",
    "    np.random.seed(0)\n",
    "\n",
    "  # generate random indices\n",
    "    random_indices = np.random.permutation(x_orig_train.shape[0])\n",
    "\n",
    "  # calculate how much to put in each partition\n",
    "    test_size = int(x_orig_train.shape[0] * p_train)\n",
    "\n",
    "  # split up the training and testing data in the same way\n",
    "    testing_indices = random_indices[:test_size] # all before test_size\n",
    "    training_indices = random_indices[test_size:] # all after test_size\n",
    "\n",
    "    x_train, y_train = x_orig_train[training_indices, :], y_orig_train[training_indices]\n",
    "    x_test, y_test = x_orig_train[testing_indices, :], y_orig_train[testing_indices]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def get_pdfrate(test=False):\n",
    "    train_data = datasets.load_svmlight_file(\"pdf_dataset/data/pdfrateB_train.libsvm\", n_features=135, zero_based=True)\n",
    "    x_orig_train, y_orig_train = train_data[0].toarray(), train_data[1]\n",
    "\n",
    "    x_train, y_train, x_test, y_test = create_partition(x_orig_train, y_orig_train)\n",
    "\n",
    "    if test:\n",
    "        test_data = datasets.load_svmlight_file(\"pdf_dataset/data/pdfrateB_test.libsvm\", n_features=135, zero_based=True)\n",
    "        x_test, y_test = test_data[0].toarray(), test_data[1]\n",
    "\n",
    "    x_train = 1 - 2*x_train\n",
    "    x_test = 1 - 2*x_test\n",
    "  \n",
    "    return cast_float(x_train), cast_int(y_train), cast_float(x_test), cast_int(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_pdfrate(test=True)\n",
    "X_train = torch.from_numpy(X_train)\n",
    "Y_train = torch.from_numpy(Y_train).long()\n",
    "X_test = torch.from_numpy(X_test)\n",
    "Y_test = torch.from_numpy(Y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8066, 135])\n",
      "torch.Size([8066])\n",
      "torch.Size([9771, 135])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SubsetRandomSampler, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_set = TensorDataset(X_train, Y_train)\n",
    "Testing_set = TensorDataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_sigmoid(input):\n",
    "    return 2*torch.sigmoid(input)-1\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(135, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = scaled_sigmoid(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x),dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,trainloader,testloader,Epoch):\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#     train_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=1)\n",
    "    for epoch in range(Epoch):  # loop over the dataset multiple times\n",
    "        PATH = \"models/pdfrate/clean_model_epoch_{}.pt\".format(epoch)\n",
    "        print(\"Epoch,\",epoch+1)\n",
    "        model.train()\n",
    "#         train_lr_scheduler.step()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        toatl = 0 \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device),data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            pred=outputs.argmax(dim=1 , keepdim=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            correct+=pred.eq(labels.view_as(pred)).sum().item()\n",
    "            \n",
    "        print(\"Training accuarcy equals:\",correct/len(trainloader.dataset))\n",
    "        print(\"Loss equals:\",running_loss)\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device),data[1].to(device)\n",
    "            outputs = model(inputs) \n",
    "            pred=outputs.argmax(dim=1 , keepdim=True)\n",
    "            correct+=pred.eq(labels.view_as(pred)).sum().item()      \n",
    "        print(\"Testing accuarcy equals:\",correct/len(testloader.dataset))\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Net()\n",
    "Train_loader = DataLoader(Training_set, batch_size=512,shuffle=True, num_workers=2)\n",
    "Test_loader = DataLoader(Testing_set, batch_size=512,shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get the saved model\n"
     ]
    }
   ],
   "source": [
    "CLEANMODEL_PATH = \"models/pdfrate/clean/clean_model_epoch_99.pt\"\n",
    "try:\n",
    "    model1.load_state_dict(torch.load(CLEANMODEL_PATH))\n",
    "    print('Get the saved model')\n",
    "except IOError:\n",
    "    print('No model in the dirctory, get a pretrained vgg16 trained on imaigenet.')\n",
    "    print(\"Start Training\")\n",
    "    train(model1,Train_loader,Test_loader,100)\n",
    "    print(\"Training complete\")\n",
    "    torch.save(model1.state_dict(),CLEANMODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stabilize_l1(model):\n",
    "    old_weights = model.fc1.weight.t()\n",
    "#     print(old_weights)\n",
    "#     print(old_weights.shape)\n",
    "    new_weights = torch.sign(old_weights)\n",
    "    old_mags = torch.linalg.norm(old_weights,ord = float('inf'),dim = 0)\n",
    "    scale_matrix = torch.diag(old_mags)\n",
    "#     print(scale_matrix.shape)\n",
    "    scale_weights = torch.matmul(new_weights,scale_matrix).t()\n",
    "    with torch.no_grad():\n",
    "        model.fc1.weight = torch.nn.Parameter(scale_weights)\n",
    "    for params in model.parameters():\n",
    "        params.requires_grad = False \n",
    "        break \n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    model.to(device)\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device),data[1].to(device)\n",
    "        outputs = model(inputs) \n",
    "        pred=outputs.argmax(dim=1 , keepdim=True)\n",
    "        correct+=pred.eq(labels.view_as(pred)).sum().item()      \n",
    "    print(\"Testing accuarcy equals:\",correct/len(testloader.dataset))\n",
    "    return correct/len(testloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbattack(model,inputs,labels):\n",
    "    brendel_attack = foolbox.attacks.L1BrendelBethgeAttack(steps=20)\n",
    "    foolbox_model = foolbox.models.PyTorchModel(model.eval(), bounds=(-2, 2))\n",
    "    criterion = foolbox.criteria.Misclassification(labels)\n",
    "    advvs = None\n",
    "    while advvs is None:\n",
    "        try:\n",
    "            advvs, _, _ = brendel_attack(foolbox_model, inputs, criterion, epsilons=None)\n",
    "        except:\n",
    "            pass \n",
    "    return advvs\n",
    "\n",
    "def Adv_Stabilize_train(model,trainloader,testloader,epoch1,epoch2,epsilon):\n",
    "    test_acc = []\n",
    "    model.to(device)\n",
    "    criter = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for oe in range(epoch1,epoch2):\n",
    "        print(\"epoch\",oe)\n",
    "        PATH = \"models/pdfrate/adv/constriant_adv_{}_model_epoch_{}.pt\".format(epsilon,oe)\n",
    "        running_loss = 0 \n",
    "        correct = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device),data[1].to(device)\n",
    "            advs = bbattack(model,inputs,labels)\n",
    "            optimizer.zero_grad()\n",
    "            diff = torch.abs(advs-inputs).detach()\n",
    "            l1flag = torch.sum(diff,1)\n",
    "            length = len(l1flag)\n",
    "            for j in range(length):\n",
    "                if l1flag[j]>epsilon:\n",
    "                    advs[j] = inputs[j]\n",
    "            outputs = model(advs)\n",
    "            model.train()\n",
    "            loss = criter(outputs, labels)\n",
    "            pred=outputs.argmax(dim=1 , keepdim=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            correct+=pred.eq(labels.view_as(pred)).sum().item()\n",
    "        print(\"Training accuarcy equals:\",correct/len(trainloader.dataset))\n",
    "        print(\"Loss equals:\",running_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device),data[1].to(device)\n",
    "            outputs = model(inputs) \n",
    "            pred=outputs.argmax(dim=1 , keepdim=True)\n",
    "            correct+=pred.eq(labels.view_as(pred)).sum().item()      \n",
    "        print(\"Testing accuarcy equals:\",correct/len(testloader.dataset))\n",
    "        test_acc.append(correct/len(testloader.dataset))\n",
    "\n",
    "\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "        \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Training accuarcy equals: 0.0007438631291842301\n",
      "Loss equals: 12.283328771591187\n",
      "Testing accuarcy equals: 0.6612424521543343\n",
      "norm: tensor(19.8202, device='cuda:0')\n",
      "Testing robustness equals: 0.0\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thisismichael/anaconda3/lib/python3.7/site-packages/eagerpy/tensor/pytorch.py:257: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return type(self)(torch.as_tensor(a, device=self.raw.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuarcy equals: 0.3730473592858914\n",
      "Loss equals: 12.835405468940735\n",
      "Testing accuarcy equals: 0.5475386347354416\n",
      "norm: tensor(26.8440, device='cuda:0')\n",
      "Testing robustness equals: 0.5191894381332515\n",
      "epoch 2\n",
      "Training accuarcy equals: 0.5388048599057773\n",
      "Loss equals: 12.213464200496674\n",
      "Testing accuarcy equals: 0.5422167638931532\n",
      "norm: tensor(26.9216, device='cuda:0')\n",
      "Testing robustness equals: 0.5298331798178283\n",
      "epoch 3\n",
      "Training accuarcy equals: 0.544011901810067\n",
      "Loss equals: 12.210780382156372\n",
      "Testing accuarcy equals: 0.5409886398526251\n",
      "norm: tensor(26.9443, device='cuda:0')\n",
      "Testing robustness equals: 0.5340292702896325\n",
      "epoch 4\n",
      "Training accuarcy equals: 0.5473592858913959\n",
      "Loss equals: 12.202835500240326\n",
      "Testing accuarcy equals: 0.5406816088424931\n",
      "norm: tensor(26.9554, device='cuda:0')\n",
      "Testing robustness equals: 0.5359738000204687\n",
      "epoch 5\n",
      "Training accuarcy equals: 0.5484750805851724\n",
      "Loss equals: 12.191369593143463\n",
      "Testing accuarcy equals: 0.540579265172449\n",
      "norm: tensor(26.9608, device='cuda:0')\n",
      "Testing robustness equals: 0.5363831747006448\n",
      "epoch 6\n",
      "Training accuarcy equals: 0.5490949665261592\n",
      "Loss equals: 12.193686962127686\n",
      "Testing accuarcy equals: 0.5403745778323611\n",
      "norm: tensor(26.9649, device='cuda:0')\n",
      "Testing robustness equals: 0.5365878620407328\n",
      "epoch 7\n",
      "Training accuarcy equals: 0.5492189437143565\n",
      "Loss equals: 12.191088914871216\n",
      "Testing accuarcy equals: 0.5403745778323611\n",
      "norm: tensor(26.9685, device='cuda:0')\n",
      "Testing robustness equals: 0.5367925493808208\n",
      "epoch 8\n",
      "Training accuarcy equals: 0.5493429209025539\n",
      "Loss equals: 12.18593579530716\n",
      "Testing accuarcy equals: 0.540169890492273\n",
      "norm: tensor(26.9721, device='cuda:0')\n",
      "Testing robustness equals: 0.5370995803909528\n",
      "epoch 9\n",
      "Training accuarcy equals: 0.5494668980907513\n",
      "Loss equals: 12.188068389892578\n",
      "Testing accuarcy equals: 0.540067546822229\n",
      "norm: tensor(26.9755, device='cuda:0')\n",
      "Testing robustness equals: 0.5370995803909528\n",
      "epoch 10\n",
      "Training accuarcy equals: 0.549714852467146\n",
      "Loss equals: 12.193108439445496\n",
      "Testing accuarcy equals: 0.540067546822229\n",
      "norm: tensor(26.9786, device='cuda:0')\n",
      "Testing robustness equals: 0.5370995803909528\n",
      "epoch 11\n",
      "Training accuarcy equals: 0.5502107612199355\n",
      "Loss equals: 12.195491671562195\n",
      "Testing accuarcy equals: 0.540067546822229\n",
      "norm: tensor(26.9815, device='cuda:0')\n",
      "Testing robustness equals: 0.5378159860812609\n",
      "epoch 12\n",
      "Training accuarcy equals: 0.5504587155963303\n",
      "Loss equals: 12.190495431423187\n",
      "Testing accuarcy equals: 0.540067546822229\n",
      "norm: tensor(26.9831, device='cuda:0')\n",
      "Testing robustness equals: 0.5380206734213488\n",
      "epoch 13\n",
      "Training accuarcy equals: 0.5505826927845276\n",
      "Loss equals: 12.194153010845184\n",
      "Testing accuarcy equals: 0.540067546822229\n",
      "norm: tensor(26.9845, device='cuda:0')\n",
      "Testing robustness equals: 0.5381230170913929\n",
      "epoch 14\n",
      "Training accuarcy equals: 0.5505826927845276\n",
      "Loss equals: 12.18987762928009\n",
      "Testing accuarcy equals: 0.540067546822229\n",
      "norm: tensor(26.9856, device='cuda:0')\n",
      "Testing robustness equals: 0.5381230170913929\n"
     ]
    }
   ],
   "source": [
    "Test_acc , Test_robust = Adv_Stabilize_train(model1,Train_loader,Test_loader,0,15,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuarcy equals: 0.6612424521543343\n",
      "Robustness: 0.00010234367004400778\n",
      "Testing accuarcy equals: 0.5475386347354416\n",
      "Robustness: 0.5197011564834715\n",
      "Testing accuarcy equals: 0.5422167638931532\n",
      "Robustness: 0.5297308361477843\n",
      "Testing accuarcy equals: 0.5409886398526251\n",
      "Robustness: 0.5340292702896325\n",
      "Testing accuarcy equals: 0.5406816088424931\n",
      "Robustness: 0.5359738000204687\n",
      "Testing accuarcy equals: 0.540579265172449\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 16872, 16873) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSocketClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mSocketClient\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5617552fe284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mTest_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 16872, 16873) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "model_test = Net()\n",
    "Test_acc =[]\n",
    "Test_robust = []\n",
    "epsilon = 50\n",
    "for it in range(15):\n",
    "    PATH = \"models/pdfrate/adv/constriant_adv_50_model_epoch_{}.pt\".format(it)\n",
    "    model_test.load_state_dict(torch.load(PATH))\n",
    "    clean_acc = evaluate(model_test,Test_loader)\n",
    "    Test_acc.append(clean_acc)\n",
    "    correct = 0 \n",
    "    for i, data in enumerate(Test_loader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device),data[1].to(device)\n",
    "        advs = bbattack(model_test,inputs,labels)\n",
    "        diff = torch.abs(advs-inputs).detach()\n",
    "        l1flag = torch.sum(diff,1)\n",
    "        length = len(l1flag)\n",
    "        for j in range(length):\n",
    "            if l1flag[j]>epsilon:\n",
    "                advs[j] = inputs[j]\n",
    "        outputs = model_test(advs) \n",
    "        pred = outputs.argmax(dim=1 , keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    robust = correct/len(Test_loader.dataset)\n",
    "    Test_robust.append(robust)\n",
    "    print(\"Robustness:\",robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = Net()\n",
    "Test_acc_sd =[]\n",
    "Test_robust_sd = []\n",
    "for it in range(15):\n",
    "    PATH = \"models/pdfrate/adv/constriant_adv_50_model_epoch_{}.pt\".format(it)\n",
    "    SD_PATH = \"models/pdfrate/stable/stable_constriant_adv_50_model_epoch_{}.pt\".format(it)\n",
    "    model_test.load_state_dict(torch.load(PATH))\n",
    "    model_sd = stabilize_l1(model_test)\n",
    "    torch.save(model_sd.state_dict(),SD_PATH)\n",
    "    clean_acc = evaluate(model_sd,Test_loader)\n",
    "    Test_acc_sd.append(clean_acc)\n",
    "    correct = 0 \n",
    "    for i, data in enumerate(Test_loader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device),data[1].to(device)\n",
    "        advs = bbattack(model_sd,inputs,labels)\n",
    "        diff = torch.abs(advs-inputs).detach()\n",
    "        l1flag = torch.sum(diff,1)\n",
    "        length = len(l1flag)\n",
    "        for j in range(length):\n",
    "            if l1flag[j]>epsilon:\n",
    "                advs[j] = inputs[j]\n",
    "        outputs = model_sd(advs) \n",
    "        pred = outputs.argmax(dim=1 , keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    robust = correct/len(Test_loader.dataset)\n",
    "    Test_robust_sd.append(robust)\n",
    "    print(\"Robustness:\",robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.linspace(1,15,15)\n",
    "Y1 = Test_robust\n",
    "Y2 = Test_robust_sd\n",
    "plt.plot(X,Y1,label='Adversarial training')\n",
    "plt.plot(X,Y2,label='Stabilization after Adversarial')\n",
    "plt.ylabel('Robustness')\n",
    "plt.xlabel('Epochs of Adversarial Training')\n",
    "plt.title(\"Robustness under L1 BrendalBethge attack less than 30\")\n",
    "plt.legend()\n",
    "plt.savefig(\"RobustRatioeps30.jpg\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = Test_acc\n",
    "Y2 = Test_acc_sd\n",
    "plt.plot(X,Y1,label='Adversarial training')\n",
    "plt.plot(X,Y2,label='Stabilization after Adversarial')\n",
    "plt.ylabel('Accuarcy')\n",
    "plt.xlabel('Epochs of Adversarial Training')\n",
    "plt.title(\"Test Accuarcy \")\n",
    "plt.legend()\n",
    "plt.savefig(\"AccRatioeps30.jpg\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = stabilize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
